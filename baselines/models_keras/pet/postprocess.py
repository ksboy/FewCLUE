import numpy as np
import pickle 
import json

label_map = {
    'bustm':
    {
        0:'1',
        1:'0'
    },
    "csldcp":
        {0: '材料科学与工程', 1: '作物学', 2: '口腔医学', 3: '药学', 4: '教育学', 5: '水利工程', 6: '理论经济学', 7: '食品科学与工程', 8: '畜牧学/兽医学', 9: '体育学', 10: '核科学与技术', 11: '力学', 12: '园艺学', 13: '水产', 14: '法学', 15: '地质学/地质资源与地质工程', 16: '石油与天然气工程', 17: '农林经济管理', 18: '信息与通信工程', 19: '图书馆、情报与档案管理', 20: '政治学', 21: '电气工程', 22: '海洋科学', 23: '民族学', 24: '航空宇航科学与技术', 25: '化学/化学工程与技术', 26: '哲学', 27: '公共卫生与预防医学', 28: '艺术学', 29: '农业工程', 30: '船舶与海洋工程', 31: '计算机科学与技术', 32: '冶金工程', 33: '交通运输工程', 34: '动力工程及工程热物理', 35: '纺织科学与工程', 36: '建筑学', 37: '环境科学与工程', 38: '公共管理', 39: '数学', 40: '物理学', 41: '林学/林业工程', 42: '心理学', 43: '历史学', 44: '工商管理', 45: '应用经济学', 46: '中医学/中药学', 47: '天文学', 48: '机械工程', 49: '土木工程', 50: '光学工程', 51: '地理学', 52: '农业资源利用', 53: '生物学/生物科学与工程', 54: '兵器科学与技术', 55: '矿业工程', 56: '大气科学', 57: '基础医学/临床医学', 58: '电子科学与技术', 59: '测绘科学与技术', 60: '控制科学与工程', 61: '军事学', 62: '中国语言文学', 63: '新闻传播学', 64: '社会学', 65: '地球物理学', 66: '植物保护'},
    "eprstmt":
        {
            0:'Negative',
            1:'Positive'
        },
    "iflytek":
        {0: '99', 1: '10', 2: '106', 3: '92', 4: '21', 5: '14', 6: '95', 7: '73', 8: '96', 9: '54', 10: '34', 11: '62', 12: '8', 13: '12', 14: '85', 15: '101', 16: '18', 17: '70', 18: '19', 19: '36', 20: '91', 21: '103', 22: '24', 23: '5', 24: '58', 25: '94', 26: '88', 27: '78', 28: '13', 29: '71', 30: '111', 31: '16', 32: '35', 33: '53', 34: '4', 35: '59', 36: '44', 37: '82', 38: '60', 39: '11', 40: '25', 41: '116', 42: '45', 43: '47', 44: '48', 45: '56', 46: '20', 47: '102', 48: '84', 49: '113', 50: '9', 51: '46', 52: '28', 53: '97', 54: '49', 55: '118', 56: '17', 57: '22', 58: '26', 59: '76', 60: '74', 61: '1', 62: '64', 63: '50', 64: '61', 65: '40', 66: '29', 67: '110', 68: '77', 69: '41', 70: '57', 71: '7', 72: '43', 73: '81', 74: '90', 75: '31', 76: '89', 77: '100', 78: '83', 79: '15', 80: '63', 81: '109', 82: '112', 83: '80', 84: '108', 85: '72', 86: '0', 87: '30', 88: '114', 89: '3', 90: '33', 91: '42', 92: '104', 93: '65', 94: '32', 95: '79', 96: '66', 97: '93', 98: '39', 99: '117', 100: '23', 101: '51', 102: '37', 103: '86', 104: '27', 105: '107', 106: '98', 107: '115', 108: '55', 109: '105', 110: '87', 111: '75', 112: '67', 113: '2', 114: '38', 115: '52', 116: '69', 117: '6', 118: '68'},
    # id2label= {0: '银行', 1: '社区服务', 2: '电商', 3: '支付', 4: '经营养成', 5: '卡牌', 6: '借贷', 7: '驾校', 8: '理财', 9: '职考', 10: '新闻', 11: '旅游资讯', 12: '公共交通', 13: '魔幻', 14: '医疗服务', 15: '影像剪辑', 16: '动作类', 17: '工具', 18: '体育竞技', 19: '小说', 20: '运动健身', 21: '相机', 22: '辅助工具', 23: '快递物流', 24: '高等教育', 25: '股票', 26: '菜谱', 27: '行车辅助', 28: '仙侠', 29: '亲子儿童', 30: '购物咨询', 31: '射击游戏', 32: '漫画', 33: '中小学', 34: '同城服务', 35: '成人教育', 36: '求职', 37: '电子产品', 38: '艺术', 39: '薅羊毛', 40: '约会社交', 41: '经营', 42: '兼职', 43: '短视频', 44: '音乐', 45: '英语', 46: '棋牌中心', 47: '摄影修图', 48: '养生保健', 49: '办公', 50: '政务', 51: '视频', 52: '论坛圈子', 53: '彩票', 54: '直播', 55: '其他', 56: '休闲益智', 57: '策略', 58: '即时通讯', 59: '汽车交易', 60: '违章', 61: '地图导航', 62: '民航', 63: '电台', 64: '语言(非英语)', 65: '搞笑', 66: '婚恋社交', 67: '社区超市', 68: '日常养车', 69: '杂志', 70: '视频教育', 71: '家政', 72: '影视娱乐', 73: '装修家居', 74: '体育咨讯', 75: '社交工具', 76: '餐饮店', 77: '美颜', 78: '问诊挂号', 79: '飞行空战', 80: '综合预定', 81: '电影票务', 82: '笔记', 83: '买房', 84: '外卖', 85: '母婴', 86: '打车', 87: '情侣社交', 88: '日程管理', 89: '租车', 90: '微博博客', 91: '百科', 92: '绘画', 93: '铁路', 94: '生活社交', 95: '租房', 96: '酒店', 97: '保险', 98: '问答交流', 99: '收款', 100: 'MOBA', 101: 'K歌', 102: '技术', 103: '减肥瘦身', 104: '工作社交', 105: '团购', 106: '记账', 107: '女性', 108: '公务员', 109: '二手', 110: '美妆美业', 111: '汽车咨询', 112: '行程管理', 113: '免费WIFI', 114: '教辅', 115: '成人', 116: '出国', 117: '婚庆', 118: '民宿短租'}
    # label2id = {'打车': 0, '美颜': 100, '影像剪辑': 101, '摄影修图': 102, '相机': 103, '绘画': 104, '二手': 105, '电商': 106, '团购': 107, '外卖': 108, '电影票务': 109, '社区服务': 10, '社区超市': 110, '购物咨询': 111, '笔记': 112, '办公': 113, '日程管理': 114, '女性': 115, '经营': 116, '收款': 117, '其他': 118, '薅羊毛': 11, '魔幻': 12, '仙侠': 13, '卡牌': 14, '飞行空战': 15, '射击游戏': 16, '休闲益智': 17, '动作类': 18, '体育竞技': 19, '地图导航': 1, '棋牌中心': 20, '经营养成': 21, '策略': 22, 'MOBA': 23, '辅助工具': 24, '约会社交': 25, '即时通讯': 26, '工作社交': 27, '论坛圈子': 28, '婚恋社交': 29, '免费WIFI': 2, '情侣社交': 30, '社交工具': 31, '生活社交': 32, '微博博客': 33, '新闻': 34, '漫画': 35, '小说': 36, '技术': 37, '教辅': 38, '问答交流': 39, '租车': 3, '搞笑': 40, '杂志': 41, '百科': 42, '影视娱乐': 43, '求职': 44, '兼职': 45, '视频': 46, '短视频': 47, '音乐': 48, '直播': 49, '同城服务': 4, '电台': 50, 'K歌': 51, '成人': 52, '中小学': 53, '职考': 54, '公务员': 55, '英语': 56, '视频教育': 57, '高等教育': 58, '成人教育': 59, '快递物流': 5, '艺术': 60, '语言(非英语)': 61, '旅游资讯': 62, '综合预定': 63, '民航': 64, '铁路': 65, '酒店': 66, '行程管理': 67, '民宿短租': 68, '出国': 69, '婚庆': 6, '工具': 70, '亲子儿童': 71, '母婴': 72, '驾校': 73, '违章': 74, '汽车咨询': 75, '汽车交易': 76, '日常养车': 77, '行车辅助': 78, '租房': 79, '家政': 7, '买房': 80, '装修家居': 81, '电子产品': 82, '问诊挂号': 83, '养生保健': 84, '医疗服务': 85, '减肥瘦身': 86, '美妆美业': 87, '菜谱': 88, '餐饮店': 89, '公共交通': 8, '体育咨讯': 90, '运动健身': 91, '支付': 92, '保险': 93, '股票': 94, '借贷': 95, '理财': 96, '彩票': 97, '记账': 98, '银行': 99, '政务': 9}
    "ocnli":
        {0: "neutral", 1:"entailment", 2:"contradiction"},
    "tnews":
        {0: "109", 1: "102", 2: "107", 3: "112", 4: "104", 5: "108", 6: "113", 7: "106", 8: "116", 9: "110", 10: "100", 11: "101", 12: "103", 13: '115', 14: "114"},    
        # id2label {0: 'news_tech', 1: 'news_entertainment', 2: 'news_car', 3: 'news_travel', 4: 'news_finance', 5: 'news_edu', 6: 'news_world', 7: 'news_house', 8: 'news_game', 9: 'news_military', 10: 'news_story', 11: 'news_culture', 12: 'news_sports', 13: 'news_agriculture', 14: 'news_stock'},
        # label2id = {'news_story': 100, 'news_culture': 101, 'news_entertainment': 102, 'news_sports': 103, 'news_finance': 104, 'news_house': 106, 'news_car': 107, 'news_edu': 108, 'news_tech': 109, 'news_military': 110, 'news_travel': 112, 'news_world': 113, 'news_stock': 114, 'news_agriculture': 115, 'news_game': 116}
    "wsc":
        {0:"true", 1:"false"}   
}

dataset = 'chid'
data = open("output/" +  dataset + '2/predict.pkl', 'rb')
labels = pickle.load(data)

lines = open('../../../datasets/'+ dataset + '/test.json',  encoding='utf-8').readlines()
result = []
for i, line in enumerate(lines):
    json_line = json.loads(line)
    if dataset in label_map:
        result.append({'id':json_line['id'], 'label': label_map[dataset][ labels[i] ]})
    else:
        result.append({'id':json_line['id'], 'label': str(labels[i])})
        # result.append({'id':json_line['id'], 'label': labels[i]})


outf = open("output/predict/"+ dataset +"_predict.json", "w")
for obj in result:
    json.dump(obj, outf, ensure_ascii=False, sort_keys=True)
    outf.write("\n")

